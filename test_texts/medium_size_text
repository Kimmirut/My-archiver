In information theory, data compression, source coding,[1] or bit-rate reduction is the
process of encoding information using fewer bits than the original representation.[2]
Any particular compression is either lossy or lossless. Lossless compression reduces bits
by identifying and eliminating statistical redundancy. No information is lost in lossless
compression. Lossy compression reduces bits by removing unnecessary or less important
information.[3] Typically, a device that performs data compression is referred to as an
encoder, and one that performs the reversal of the process (decompression) as a decoder.
The process of reducing the size of a data file is often referred to as data compression.
In the context of data transmission, it is called source coding: encoding is done at
the source of the data before it is stored or transmitted.[4] Source coding should not
be confused with channel coding, for error detection and correction or line coding, the
means for mapping data onto a signal.
Compression is useful because it reduces the resources required to store and transmit
data. Computational resources are consumed in the compression and decompression processes.
Data compression is subject to a space-time complexity trade-off. For instance, a
compression scheme for video may require expensive hardware for the video to be
decompressed fast enough to be viewed as it is being decompressed, and the option to
decompress the video in full before watching it may be inconvenient or require additional
storage. The design of data compression schemes involves trade-offs among various factors,
including the degree of compression, the amount of distortion introduced (when using lossy
data compression), and the computational resources required to compress and decompress the
data.
Genetics compression algorithms are the latest generation of lossless algorithms that
compress data (typically sequences of nucleotides) using both conventional compression
algorithms and genetic algorithms adapted to the specific datatype. In 2012, a team of
scientists from Johns Hopkins University published a genetic compression algorithm that
does not use a reference genome for compression. HAPZIPPER was tailored for HapMap data
and achieves over 20-fold compression (95% reduction in file size), providing 2- to 4-fold
better compression and is less computationally intensive than the leading general-purpose
compression utilities. For this, Chanda, Elhaik, and Bader introduced MAF-based encoding
(MAFE), which reduces the heterogeneity of the dataset by sorting SNPs by their minor
allele frequency, thus homogenizing the dataset.[77] Other algorithms developed in 2009
and 2013 (DNAZip and GenomeZip) have compression ratios of up to 1200-foldâ€”allowing 6
billion basepair diploid human genomes to be stored in 2.5 megabytes (relative to a
reference genome or averaged over many genomes).[78][79] For a benchmark in
genetics/genomics data compressors, see
